Created json args file	-> dumps\pt_20221129_220703\args.json

Python Version: 3.7.15
PyTorch Version: 1.13.0+cu116
Torchvision Version: 0.14.0+cu116

ARCHITECTURE:
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=5, bias=True)
  )
)

features.0.weight         requires_grad = False
features.0.bias           requires_grad = False
features.3.weight         requires_grad = False
features.3.bias           requires_grad = False
features.6.weight         requires_grad = False
features.6.bias           requires_grad = False
features.8.weight         requires_grad = False
features.8.bias           requires_grad = False
features.10.weight        requires_grad = False
features.10.bias          requires_grad = False
classifier.1.weight       requires_grad = False
classifier.1.bias         requires_grad = False
classifier.4.weight       requires_grad = False
classifier.4.bias         requires_grad = False
classifier.6.weight       requires_grad = True
classifier.6.bias         requires_grad = True
RUNNING ARGS:
{
    "seed": 2020,
    "batch_size": 14,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0,
    "optimizer": "sgdm",
    "input_size": 224,
    "display_images": false,
    "pretrained": true,
    "save": true,
    "device": "cpu",
    "t_start": "20221129_220703"
}


Created stats file	-> dumps\pt_20221129_220703\stats.csv

TRAINING 100 EPOCHS...

     epoch train_loss  train_acc   train_f1   val_loss    val_acc     val_f1
         0   0.628716    0.76875   0.768174   0.204336      0.915   0.914692
         1   0.354512     0.8775   0.877969   0.098645       0.96   0.959844
         2   0.414498    0.84375   0.843659   0.100497       0.95   0.950559
         3   0.369455    0.86625   0.865865   0.091985      0.965   0.965158
         4   0.310122     0.8825   0.882578   0.095947      0.955   0.954511
         5   0.347584    0.89125   0.891148   0.088754       0.96   0.960178
         6   0.274248     0.8925   0.892643   0.091365      0.955   0.954865
         7   0.329522    0.87875   0.878456   0.090054      0.965   0.965722
         8   0.346374    0.87125   0.871701   0.041812      0.995   0.994999
         9   0.336919      0.895   0.895887   0.063179       0.98   0.979776
        10   0.277184    0.90875   0.908766   0.041816       0.99   0.989934
        11   0.336203     0.8725    0.87331   0.086566      0.965   0.965722
        12   0.330059     0.8925   0.892914   0.045444      0.985   0.985176
        13   0.335732    0.87875   0.878798   0.055367      0.975   0.974983
        14   0.305155     0.8975     0.8976   0.039202       0.99   0.990059
        15   0.299911    0.88625   0.886248   0.036748       0.99   0.990059
        16   0.284125    0.89625   0.896273   0.040272      0.975   0.974932
        17   0.301196    0.89625   0.896709   0.058424       0.97   0.970288
        18   0.261212      0.905   0.905283   0.031854      0.985   0.985176
        19   0.259486      0.895   0.895221   0.033344      0.985   0.985176
        20   0.250145    0.92125   0.921184   0.022967      0.995   0.994999
        21    0.28403      0.905   0.905215   0.033865      0.995   0.994999
        22   0.247441     0.9125   0.912682   0.022752        1.0        1.0
        23   0.227001    0.92625   0.926119   0.043558       0.98   0.980115
        24   0.294342        0.9   0.899702    0.03927      0.985   0.985111
        25   0.270958     0.9075   0.907713   0.039522       0.98   0.980285
        26   0.309679    0.88625   0.886285   0.045524       0.98   0.980285
        27    0.28858    0.90125    0.90147   0.028862      0.995   0.994999
        28   0.250294    0.91125    0.91091   0.044443       0.99   0.989994
        29    0.23863     0.9225   0.922629   0.034831      0.985   0.984994
        30   0.252878    0.90875    0.90852   0.046268       0.98   0.980285
        31   0.238331     0.9125   0.912608   0.029643       0.99   0.990059
        32   0.356475    0.88125   0.880971   0.022202        1.0        1.0
        33   0.273871    0.89875   0.898755   0.023053      0.995   0.994999
        34   0.280079      0.905   0.905211    0.04379       0.98   0.980152
        35   0.275735    0.89375   0.893988   0.021146       0.99   0.989999
        36   0.258201      0.915   0.915014   0.015122        1.0        1.0
        37   0.217382      0.935   0.934939   0.026804        1.0        1.0
        38   0.307916    0.90125    0.90125   0.024024       0.99   0.990059
        39   0.312956     0.9025   0.902499   0.030524      0.985   0.985111
        40   0.256808    0.91125   0.911415   0.069088      0.975   0.974902
        41   0.284524      0.895   0.894908   0.032083       0.99   0.990059
        42   0.267956     0.8975   0.897616   0.028525      0.995   0.994999
        43   0.286055       0.91   0.910511   0.022835      0.995   0.994999
        44   0.256935     0.9075   0.907553   0.037049       0.99   0.989934
        45   0.282439    0.90375   0.903719   0.050417       0.99   0.989934
        46   0.257516      0.915   0.915034   0.020217      0.995   0.994999
        47   0.242753    0.91375   0.913489   0.031631      0.985   0.985111
        48   0.284413    0.88625   0.886732   0.020776        1.0        1.0
        49   0.264611    0.91875   0.918872   0.020642        1.0        1.0
        50   0.311438        0.9   0.900127   0.015851      0.995   0.994999
        51   0.242191      0.915   0.915226   0.014423        1.0        1.0
        52   0.268815     0.9075   0.907589   0.021219        1.0        1.0
        53   0.238687     0.9075   0.907865   0.022843      0.995   0.994999
        54   0.303176    0.89625    0.89635   0.019806      0.995   0.994999
        55   0.267077    0.89875   0.899166   0.023282      0.995   0.994999
        56   0.264352       0.91    0.91038   0.025048      0.995   0.994999
        57   0.250327      0.915   0.915014   0.018883       0.99   0.989999
        58   0.281461    0.89375   0.893701     0.0182      0.995   0.994999
        59   0.258509    0.90875   0.908599    0.01649      0.995   0.994999
        60     0.2724    0.90875   0.909132   0.019104      0.995   0.994999
        61    0.25394    0.92125   0.921212   0.013262        1.0        1.0
        62   0.269802    0.91375   0.913926   0.022502        1.0        1.0
        63    0.29481    0.90875   0.908736   0.020738      0.995   0.994999
        64   0.217172       0.92   0.919829   0.019718      0.995   0.994999
        65   0.247736    0.91375   0.914155   0.012915      0.995   0.994999
        66   0.198398    0.93375   0.933747   0.015118        1.0        1.0
        67   0.263261    0.91375   0.914033   0.037704       0.98   0.980152
        68   0.259342      0.905   0.905218   0.024844       0.99   0.989999
        69   0.211535    0.92375   0.923807   0.020486        1.0        1.0
        70    0.24501    0.91125   0.911316   0.040025      0.975    0.97538
        71   0.302343    0.90625   0.906745   0.013048        1.0        1.0
        72   0.227948    0.91625   0.916444   0.016679      0.995   0.994999
        73     0.2624      0.915   0.915134   0.010301        1.0        1.0
        74   0.255415    0.91375   0.913647   0.066523       0.96   0.961035
        75   0.225406    0.91875   0.918626   0.024262       0.99   0.989934
        76   0.254796     0.9125   0.912565   0.025577      0.985   0.985111
        77   0.259337    0.90625   0.906178   0.066944       0.98    0.97995
        78   0.253373    0.92875    0.92873   0.010673        1.0        1.0
        79   0.224673       0.92     0.9201   0.041197       0.98    0.97995
        80   0.281594    0.91375   0.913818   0.025736      0.985   0.985111
        81   0.302663        0.9   0.900023   0.012934      0.995   0.994999
        82   0.275005    0.90375   0.903748   0.022427      0.995   0.994999
        83   0.244945       0.91   0.910018   0.022823      0.995   0.994999
        84   0.219728    0.91375   0.913786   0.069347      0.975   0.974902
        85   0.313682       0.91   0.910013   0.029614      0.985   0.985111
        86   0.291091        0.9   0.900298   0.019547      0.995   0.994999
        87   0.235793    0.91875   0.918869   0.021173      0.995   0.994999
        88   0.277472    0.91125   0.910841   0.042738      0.975   0.975087
        89   0.282082    0.90375   0.903537   0.054215       0.97   0.970594
        90   0.238064    0.91375   0.914269   0.011165        1.0        1.0
        91   0.352742    0.88375   0.883875   0.110889      0.955   0.955021
        92    0.26582       0.91   0.909796   0.012026        1.0        1.0
        93   0.262086    0.90875     0.9087   0.021732       0.99   0.990059
        94   0.256303    0.90875   0.909017   0.020005      0.995   0.994999
        95   0.237688     0.9225    0.92252   0.017116        1.0        1.0
        96   0.177065     0.9375   0.937448   0.014285        1.0        1.0
        97   0.319779    0.89625   0.896346   0.013911      0.995   0.994999
        98   0.263578      0.905   0.905192   0.048261       0.98   0.980152
        99   0.251278       0.91   0.910236    0.01723      0.995   0.994999

Training completed in 13m 27s

Saved best checkpoint	-> models\pt_20221129_220703_1.000000.pth

TESTING...
247/250 predictions are correct -> Test acc: 0.988000   f1: 0.987996

